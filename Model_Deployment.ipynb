{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization, Input, Dropout, RandomFlip, Resizing, Rescaling\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, FalsePositives, FalseNegatives, TruePositives, TrueNegatives, Precision, Recall, AUC\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers  import L2, L1\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  tf.data.Dataset.load(r\"C:\\Users\\jorge\\DataSets\\MalariaDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"infected\", \"uninfected\"]\n",
    "\n",
    "CONFIGURATION = {\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IM_HEIGHT\" : 256,\n",
    "    \"IM_WIDTH\" : 256,\n",
    "    \"NORMALIZATION\" : 255,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"N_EPOCHS\": 2,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\": 0.0,\n",
    "    \"N_FILTERS\": 6,\n",
    "    \"KERNEL_SIZE\": 3,\n",
    "    \"N_STRIDES\": 1,\n",
    "    \"POOL_SIZE\": 2,\n",
    "    \"N_DENSE_1\": 100,\n",
    "    \"N_DENSE_2\": 10,\n",
    "    \"NUM_CLASSES\":2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layers = Sequential([\n",
    "    Resizing(CONFIGURATION[\"IM_HEIGHT\"], CONFIGURATION[\"IM_WIDTH\"], interpolation=\"bilinear\"),           # Interpolation method used: Bilinear\n",
    "    Rescaling(1./CONFIGURATION[\"NORMALIZATION\"])])                                                       # Normalization factor            \n",
    "\n",
    "@tf.function\n",
    "def map_fn(images, labels):\n",
    "    processed_images = resize_rescale_layers(images)\n",
    "    return processed_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(map_fn, num_parallel_calls = tf.data.AUTOTUNE).batch(CONFIGURATION[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO, seed = None):\n",
    "  DATASET_SIZE = len(dataset)\n",
    "\n",
    "  if seed is not None:\n",
    "    dataset = dataset.shuffle(DATASET_SIZE, seed=seed)\n",
    "  else:\n",
    "    dataset = dataset.shuffle(DATASET_SIZE)\n",
    "\n",
    "  train_dataset = dataset.take(int(TRAIN_RATIO*DATASET_SIZE))\n",
    "\n",
    "  val_test_dataset = dataset.skip(int(TRAIN_RATIO*DATASET_SIZE))\n",
    "  val_dataset = val_test_dataset.take(int(VAL_RATIO*DATASET_SIZE))\n",
    "\n",
    "  test_dataset = val_test_dataset.skip(int(VAL_RATIO*DATASET_SIZE))\n",
    "  return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_dataset\n",
    "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    val_dataset\n",
    "    .shuffle(buffer_size = 32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_callback = CSVLogger(r\"C:\\Users\\jorge\\OneDrive\\Im√°genes\\Documentos\\Malaria Detection\\Model logs\\logs.csv\", \n",
    "                        separator = \",\",\n",
    "                        append = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor = \"val_loss\",     \n",
    "    min_delta = 0,                        #An absolute change of less than min_delta, will count as no improvement.\n",
    "    patience = 5,                         #Number of epochs with no improvement after which training will be stopped.\n",
    "    verbose = 0,          \n",
    "    mode = \"auto\",                        #In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing.\n",
    "    baseline = None,                      #Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.\n",
    "    restore_best_weights = False          #Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 1:\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        learning_rate = lr * tf.math.exp(-0.1)\n",
    "    return learning_rate\n",
    "  \n",
    "scheduler_callback = LearningRateScheduler(scheduler, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plateau_callback = ReduceLROnPlateau(monitor = \"val_accuracy\", \n",
    "                                     factor = 0.3, \n",
    "                                     patience = 5, \n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom loss class\n",
    "\n",
    "This is done in case the custom loss functions needs to be modified. For now it will remain the same, but is a good practice to define this function in case is necessary to change it in future phases of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBCE(tf.keras.losses.Loss):\n",
    "  def __init__(self, FACTOR):\n",
    "    super(CustomBCE, self).__init__()\n",
    "    self.FACTOR = FACTOR\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "    bce = BinaryCrossentropy()\n",
    "    return bce(y_true, y_pred)* self.FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom metrics class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAccuracy(tf.keras.metrics.Metric):\n",
    "  def __init__(self, name = 'Custom_Accuracy', FACTOR = 1):\n",
    "    super(CustomAccuracy, self).__init__()\n",
    "    self.FACTOR = FACTOR\n",
    "    self.accuracy = self.add_weight(name = name, initializer = 'zeros')\n",
    "\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "    output = binary_accuracy(tf.cast(y_true, dtype = tf.float32), y_pred)*self.FACTOR\n",
    "    self.accuracy.assign(tf.math.count_nonzero(output, dtype = tf.float32)/tf.cast(len(output), dtype = tf.float32))\n",
    "\n",
    "  def result(self):\n",
    "    return self.accuracy\n",
    "\n",
    "  def reset_states(self):\n",
    "    self.accuracy.assign(0.)\n",
    "\n",
    "FACTOR = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [TruePositives(name='tp'),FalsePositives(name='fp'), TrueNegatives(name='tn'), FalseNegatives(name='fn'), \n",
    "            BinaryAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extractor (sequential API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_seq_model = tf.keras.Sequential([\n",
    "    InputLayer(input_shape = (CONFIGURATION[\"IM_HEIGHT\"], CONFIGURATION[\"IM_WIDTH\"], 3)),\n",
    "\n",
    "    Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D (pool_size = 2, strides= 2),\n",
    "\n",
    "    Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D (pool_size = 2, strides= 2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 6)       168       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 254, 254, 6)       24        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 127, 127, 6)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 16)      880       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 125, 125, 16)      64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1136 (4.44 KB)\n",
      "Trainable params: 1092 (4.27 KB)\n",
      "Non-trainable params: 44 (176.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callable Model (functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_input = Input(shape = (CONFIGURATION[\"IM_HEIGHT\"], CONFIGURATION[\"IM_WIDTH\"], 3), name = \"Input Image\")\n",
    "\n",
    "x = feature_extractor_seq_model(func_input)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation = \"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(10, activation = \"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "func_output = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "lenet_model = Model(func_input, func_output, name = \"Lenet_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Lenet_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input Image (InputLayer)    [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 62, 62, 16)        1136      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 61504)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               6150500   \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 10)                40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6153097 (23.47 MB)\n",
      "Trainable params: 6152833 (23.47 MB)\n",
      "Non-trainable params: 264 (1.03 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model.compile(optimizer = Adam(learning_rate = CONFIGURATION['LEARNING_RATE']),\n",
    "      loss = BinaryCrossentropy(CustomBCE(FACTOR)),        # Custom loss class defined above\n",
    "      metrics = metrics)                             # Metriics list defined above. For now the class that was defined for the accuracy won¬¥t be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:5805: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/689 [===>..........................] - ETA: 8:26 - loss: 0.6345 - tp: 1202.0000 - fp: 613.0000 - tn: 1086.0000 - fn: 587.0000 - accuracy: 0.6560 - precision: 0.6623 - recall: 0.6719 - auc: 0.7063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jorge\\OneDrive\\Im√°genes\\Documentos\\Malaria Proyect\\Model_Deployment.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m lenet_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     validation_data \u001b[39m=\u001b[39;49m val_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs \u001b[39m=\u001b[39;49m CONFIGURATION[\u001b[39m'\u001b[39;49m\u001b[39mN_EPOCHS\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [csv_callback, es_callback, scheduler_callback, plateau_callback]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jorge/OneDrive/Im%C3%A1genes/Documentos/Malaria%20Proyect/Model_Deployment.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = lenet_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    epochs = CONFIGURATION['N_EPOCHS'],\n",
    "    verbose = 1,\n",
    "    callbacks = [csv_callback, es_callback, scheduler_callback, plateau_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biblbiography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler      -       https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html    \n",
    "BinaryCrossEntropy           -       https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
